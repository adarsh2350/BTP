import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from pprint import pprint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
df = pd.read_csv('BTP_4.csv')
df.sample(5)
features_input = np.array(['Surface Tension','Contact Angle','viscosity'])
features_output = np.array(['TA','AW','TL','FL'])
### Random Forest Regression
validation = pd.DataFrame(columns=['Title','mse','mae','rmse','r2'])

for feature1 in features_input:
    for feature2 in features_output:
        x = feature1
        y = feature2
        X=df[x]
        Y=df[y]
        
        k=5
        kf = KFold(n_splits=k, shuffle=True, random_state=40)
        mse_scores = []
        mae_scores = []
        rmse_scores = []
        r2_scores = []
        model = RandomForestRegressor(n_estimators=100)
        

        min_mse = 1e9
        x_train_scat = pd.DataFrame()
        y_train_scat = pd.Series(dtype='float64')
        x_train_plot = pd.Series(dtype='float64')
        y_train_plot = np.array([])
        x_test_scat = pd.DataFrame()
        y_test_scat = pd.Series(dtype='float64')
        y_test_plot = np.array([])

        for train_index, test_index in kf.split(X):
            
            x_train, x_test = X[train_index], X[test_index]
            y_train, y_test = Y[train_index], Y[test_index]
            x_train = pd.DataFrame(x_train, columns=[x])
            x_test = pd.DataFrame(x_test, columns=[x])
     
            model.fit(x_train, y_train)
            y_pred = model.predict(x_test)
            y_pred_train = model.predict(x_train)
            
            mse_scores.append(mean_squared_error(y_test, y_pred))
            mae_scores.append(mean_absolute_error(y_test, y_pred))
            rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))
            r2_scores.append(r2_score(y_test,y_pred))

            if mean_squared_error(y_test, y_pred) < min_mse:
                min_X = X
                x_train_scat = x_train
                y_train_scat = y_train
                x_train_plot = X[train_index]
                y_train_plot = y_pred_train
                x_test_scat = x_test
                y_test_scat = y_test
                y_test_plot = y_pred


        plt.scatter(x_train_scat,y_train_scat,color='red')
        plt.scatter(x_train_plot,y_train_plot,color='blue')
        plt.title(y+' vs '+x+' (Training data)')
        plt.xlabel(feature1)
        plt.ylabel(feature2)
        plt.show()

        plt.scatter(x_test_scat,y_test_scat,color='green')
        plt.scatter(x_test_scat,y_test_plot,color='orange')
        plt.title(y+' vs '+x+' (Test data)')
        plt.xlabel(feature1)
        plt.ylabel(feature2)
        plt.show()
        
        mse = np.mean(mse_scores)
        mae = np.mean(mae_scores)
        rmse = np.mean(rmse_scores)
        r2 = np.mean(r2_scores)
        
        val = {'Title':[feature1+' vs '+feature2],
                      'mse':[mse],
                      'mae':[mae],
                      'rmse':[rmse],
                      'r2':[r2]}
        val = pd.DataFrame(val)
        validation = pd.concat([validation, val], ignore_index=True)
validation
X = df[features_input]
Y = df[features_output]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
rf = RandomForestRegressor()

print('Parameters currently in use:\n')
pprint(rf.get_params())
bootstrap = [True, False]
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]


random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

pprint(random_grid)
rf = RandomForestRegressor(random_state = 42)

rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,
                        n_iter = 100, scoring='neg_mean_absolute_error', 
                        cv = 3, verbose=2, random_state=42, n_jobs=-1,
                        return_train_score=True)
rf_random.fit(X_train, Y_train)
rf_random.best_params_
best_random = rf_random.best_estimator_
Y_pred = best_random.predict(X_test)
print('mean_squared_error : ',mean_squared_error(Y_test, Y_pred))
print('mean_absolute_error : ',mean_absolute_error(Y_test, Y_pred))
print('mean_squared_error : ',np.sqrt(mean_squared_error(Y_test, Y_pred)))
print('r2_score : ',r2_score(Y_test, Y_pred))
param_grid = {
    'bootstrap': [False],
    'max_depth': [50, 60, 70, 80, 90],
    'min_samples_leaf': [1, 2, 3, 4],
    'min_samples_split': [2, 4, 6, 8],
    'n_estimators': [200, 300, 400, 500, 600]
}


rf = RandomForestRegressor(random_state = 42)

grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)

grid_search.fit(X_train, Y_train)
grid_search.best_params_
best_grid = grid_search.best_estimator_
Y_pred = best_grid.predict(X_test)
Y_pred_train = best_grid.predict(X_train)
print('mean_squared_error : ',mean_squared_error(Y_test, Y_pred))
print('mean_absolute_error : ',mean_absolute_error(Y_test, Y_pred))
print('mean_squared_error : ',np.sqrt(mean_squared_error(Y_test, Y_pred)))
print('r2_score : ',r2_score(Y_test, Y_pred))
Y_pred = pd.DataFrame(Y_pred, columns=features_output)
Y_pred_train = pd.DataFrame(Y_pred_train, columns=features_output)
for feature1 in features_input:
    for feature2 in features_output:
        x = feature1
        y = feature2
        

        x_train_scat = X_train[x]
        y_train_scat = Y_train[y]
        y_train_plot = Y_pred_train[y]
        x_test_scat = X_test[x]
        y_test_scat = Y_test[y]
        y_test_plot = Y_pred[y]


        plt.scatter(x_train_scat,y_train_scat,color='red')
        plt.scatter(x_train_scat,y_train_plot,color='blue')
        plt.title(y+' vs '+x+' (Training data)')
        plt.xlabel(feature1)
        plt.ylabel(feature2)
        plt.show()

        plt.scatter(x_test_scat,y_test_scat,color='green')
        plt.scatter(x_test_scat,y_test_plot,color='orange')
        plt.title(y+' vs '+x+' (Test data)')
        plt.xlabel(feature1)
        plt.ylabel(feature2)
        plt.show()
validation = pd.DataFrame(columns=['Title','mse','mae','rmse','r2'])

mse_scores = []
mae_scores = []
rmse_scores = []
r2_scores = []

for feature in features_output:

    mse = mean_squared_error(Y_test[feature], Y_pred[feature])
    mae = mean_absolute_error(Y_test[feature], Y_pred[feature])
    rmse = np.sqrt(mean_squared_error(Y_test[feature], Y_pred[feature]))
    r2 = r2_score(Y_test[feature], Y_pred[feature])

    val = {'Title':[feature],
            'mse':[mse],
            'mae':[mae],
            'rmse':[rmse],
            'r2':[r2]}
    
    val = pd.DataFrame(val)
    validation = pd.concat([validation, val], ignore_index=True)
validation
